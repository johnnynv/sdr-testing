{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Streaming Data to RAG\n",
    "\n",
    "Traditional retrieval-augmented generation (RAG) systems rely on static data ingested in batches, which limits their ability to support time-critical use cases like emergency response or live monitoring. These situations require immediate access to dynamic data sources such as sensor feeds or radio signals. \n",
    "\n",
    "The Streaming Data to RAG developer example solves this by enabling RAG systems to process live data streams in real-time. It features a GPU-accelerated software-defined radio (SDR) pipeline that continuously captures radio frequency (RF) signals, transcribes them into searchable text, embeds, and indexes them in real time. This live data is then fed to a large language model (LLM), allowing context-aware queries over dynamic streams.\n",
    "\n",
    "Designed for scalability across edge and cloud environments, this reference example unlocks real-time situational awareness for use cases like spectrum monitoring, intelligence gathering, and other mission-critical applications‚Äîwhile retaining RAG‚Äôs strengths in delivering accurate, relevant results. \n",
    "\n",
    "\n",
    "## Full Architecture Diagram\n",
    "<img src=\"../docs/arch-diagram.png\" alt=\"Architecture Diagram\" style=\"max-width: 600px; width: 100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Submodule Initialization\n",
    "\n",
    "This blueprint utilizes 2 open-source NVIDIA repositories that have been augmented for this workflow:\n",
    "- NeMo Agent Toolkit UI: Open-source repository used as the UI\n",
    "- Context-Aware RAG: Open-source RAG repository originally shown in the [VSS Blueprint](https://build.nvidia.com/nvidia/video-search-and-summarization)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Submodule 'external/NeMo-Agent-Toolkit-UI' (https://github.com/NVIDIA/NeMo-Agent-Toolkit-UI.git) registered for path '../external/NeMo-Agent-Toolkit-UI'\n",
      "Submodule 'external/context-aware-rag' (https://github.com/NVIDIA/context-aware-rag.git) registered for path '../external/context-aware-rag'\n",
      "Cloning into '/home/deustice/Projects/streaming-data-to-rag/external/NeMo-Agent-Toolkit-UI'...\n",
      "Cloning into '/home/deustice/Projects/streaming-data-to-rag/external/context-aware-rag'...\n",
      "Submodule path '../external/NeMo-Agent-Toolkit-UI': checked out 'b9ccc559efbd0ac378269da1d3427a5954bd5f8b'\n",
      "Submodule path '../external/context-aware-rag': checked out '04bb45b89598fb47d253fb15d50a0acb444ef95d'\n"
     ]
    }
   ],
   "source": [
    "!git submodule update --init --recursive"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Docker Login\n",
    "\n",
    "To pull images required by the blueprint from NGC, you must first [authenticate Docker with nvcr.io](https://docs.nvidia.com/launchpad/ai/base-command-coe/latest/bc-coe-docker-basics-step-02.html#logging-in-to-ngc-on-a-workstation). Paste your NVIDIA API key in the cell below. If you don't have an API key, one can be obtained for free with an NVIDIA developer account at [build.nvidia.com](https://build.nvidia.com)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Login Succeeded\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import subprocess\n",
    "import os\n",
    "\n",
    "# ADD YOUR API KEY\n",
    "NVIDIA_API_KEY = \"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\"\n",
    "os.environ['NVIDIA_API_KEY'] = NVIDIA_API_KEY\n",
    "os.environ['NGC_API_KEY'] = NVIDIA_API_KEY\n",
    "\n",
    "# Authenticate local Docker with NGC\n",
    "cmd = f\"echo {NVIDIA_API_KEY} | docker login nvcr.io -u '$oauthtoken' --password-stdin\"\n",
    "result = subprocess.run(cmd, shell=True, capture_output=True, text=True)\n",
    "print(result.stdout)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model Directory\n",
    "\n",
    "For the NIMs that will be deployed locally, we will set up a directory to use as a cache for model weights, so that we don't need to re-download the models each time the NIMs are restarted:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Setting MODEL_DIRECTORY to '/home/deustice/Projects/streaming-data-to-rag/models'\n"
     ]
    }
   ],
   "source": [
    "# Set up local directory to cache model weights downloaded from NGC\n",
    "import os\n",
    "os.environ['MODEL_DIRECTORY'] = os.path.abspath(os.path.join(os.getcwd(), \"../models\"))\n",
    "!echo \"Setting MODEL_DIRECTORY to '$MODEL_DIRECTORY'\"\n",
    "!mkdir -p $MODEL_DIRECTORY\n",
    "!chmod 777 -R $MODEL_DIRECTORY"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Expose Frontend\n",
    "\n",
    "In order to access the frontend, ensure that port 3000 is exposed on whatever instance the blueprint is running on.\n",
    "\n",
    "For Brev deployments, go to your instance page on [brev.nvidia.com](brev.nvidia.com) and scroll down to \"Using Ports\", then expose port 3000."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## File Replay Setup\n",
    "\n",
    "This notebook is configured to run via file replay, which is used when a physical antenna & SDR are not hooked up to the system. The file replay service reads audio from file, FM modulates it, and streams the resulting baseband I/Q samples over UDP.\n",
    "\n",
    "Here, we only set the files to replay (which **must** be located in `src/file-replay/files/sample_files`) and the max replay time. There are other configurable options to the replay outlined in the README.\n",
    "\n",
    "Some sample files from [NVIDIA's AI Podcast](https://ai-podcast.nvidia.com/) have been included here for demo purposes. The audio has been clipped for brevity, and names and voices have been modified for privacy.\n",
    "\n",
    "You may also upload your own audio files to explore the Streaming Data to RAG developer example on custom content. Any audio file that can be loaded with [`librosa.load`](https://librosa.org/doc/latest/generated/librosa.load.html) can be used.\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b>\n",
    "    \n",
    "In the [Holoscan SDR's parameter file](../src/software-defined-radio/params.yaml), be sure to check that the following options match or do not contradict your replay configuration:\n",
    "    \n",
    "- Sample rate (Hz): `sensor.sample_rate`\n",
    "- UDP port: `network_rx.dst_port`\n",
    "- UDP max payload size: `network_rx.max_payload_size`\n",
    "- Number files / channels: `channelizer.num_channels`\n",
    "- Channel spacing (Hz): `channelizer.channel_spacing`\n",
    "</div>\n",
    "\n",
    "### Set Environment Variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "env: REPLAY_FILES=sample_files/ai_gtc_1.mp3, sample_files/ai_gtc_2.mp3, sample_files/ai_gtc_3.mp3\n",
      "env: REPLAY_TIME=3600\n",
      "env: REPLAY_MAX_FILE_SIZE=50\n"
     ]
    }
   ],
   "source": [
    "# Replay files must be comma-separated and located in `src/file-replay/files`\n",
    "%env REPLAY_FILES=sample_files/ai_gtc_1.mp3, sample_files/ai_gtc_2.mp3, sample_files/ai_gtc_3.mp3\n",
    "\n",
    "# Max time to replay in seconds - set to 1 hour\n",
    "%env REPLAY_TIME=3600\n",
    "\n",
    "# Set the default max file size to 50MB\n",
    "%env REPLAY_MAX_FILE_SIZE=50"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check file sizes\n",
    "\n",
    "To prevent issues running out of GPU memory, the default max file size for audio files is 50 MB. This is configured by the `REPLAY_MAX_FILE_SIZE` environment variable set in the cell above. The cell below will check the configured replay files and increase the max size to accommodate the largest one. If it is increased, be mindful of the available GPU memory, which can always be checked with `nvidia-smi` from the command line."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking file sizes for REPLAY_FILES:\n",
      "  ‚úÖ 'sample_files/ai_gtc_1.mp3': 26.03 MB\n",
      "  ‚úÖ 'sample_files/ai_gtc_2.mp3': 30.18 MB\n",
      "  ‚úÖ 'sample_files/ai_gtc_3.mp3': 24.30 MB\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Get REPLAY_FILES from environment and split and strip whitespace\n",
    "replay_files_env = os.environ.get(\"REPLAY_FILES\", \"\")\n",
    "replay_files = [f.strip() for f in replay_files_env.split(\",\") if f.strip()]\n",
    "\n",
    "print(\"Checking file sizes for REPLAY_FILES:\")\n",
    "original_max_size_mb = float(os.environ.get(\"REPLAY_MAX_FILE_SIZE\"))\n",
    "file_directory = os.path.join(\"..\", \"src\", \"file-replay\", \"files\")\n",
    "for fname in replay_files:\n",
    "    file_path = os.path.join(file_directory, fname)  # Files are relative to src/file-replay/files\n",
    "    file_exists = os.path.exists(file_path)\n",
    "\n",
    "    # Check the file size and compare to the configured max size\n",
    "    max_size_mb = float(os.environ.get(\"REPLAY_MAX_FILE_SIZE\"))\n",
    "    file_size_mb = os.path.getsize(file_path) / (1024 * 1024)\n",
    "    under_size_limit = file_size_mb < max_size_mb\n",
    "\n",
    "    if file_exists and under_size_limit:\n",
    "        # File exists and is under the max size limit\n",
    "        print(f\"  ‚úÖ '{fname}': {file_size_mb:.2f} MB\")\n",
    "    elif not under_size_limit:\n",
    "        # File exists and exceeds the max size limit\n",
    "        new_limit = file_size_mb + 1\n",
    "        print(\n",
    "            f\"  üöß '{fname}' ({file_size_mb:.2f} MB) exceeds {max_size_mb:0.2f} MB limit... \"\n",
    "            f\"REPLAY_MAX_FILE_SIZE --> {new_limit:0.2f} MB\"\n",
    "        )\n",
    "        os.environ[\"REPLAY_MAX_FILE_SIZE\"] = str(new_limit)\n",
    "    else:\n",
    "        # File does not exist\n",
    "        print(f\"  ‚ùå '{fname}': File not found at {file_path}\")\n",
    "        raise RuntimeError(f\"File {fname} not found at {file_path}\")\n",
    "\n",
    "max_size_mb = float(os.environ.get(\"REPLAY_MAX_FILE_SIZE\"))\n",
    "if original_max_size_mb < max_size_mb:\n",
    "    print(\"\\n----------------------------------------------------------------------------\")\n",
    "    print(f\"üöß WARNING: REPLAY_MAX_FILE_SIZE was increased to {max_size_mb:0.2f} MB to accommodate the largest file.\")\n",
    "    print(\"üöß Be aware that the replay container holds all audio files in GPU memory while running.\")\n",
    "    print(\"üöß For higher file sizes, be mindful of the availablility of GPU memory, which can be checked with `nvidia-smi`.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Validate Required Environment Variables Are Set\n",
    "\n",
    "The below cell will throw and error if there was an issue setting either of the required ENV variables above:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Check NVIDIA_API_KEY\n",
    "api_key = os.environ.get(\"NVIDIA_API_KEY\")\n",
    "if not api_key or api_key == \"nvapi-your-api-key\":\n",
    "    raise RuntimeError(\n",
    "        \"NVIDIA_API_KEY environment variable is not set or is set to the default placeholder value. \"\n",
    "        \"Please set your NVIDIA API key from build.nvidia.com.\"\n",
    "    )\n",
    "\n",
    "# Check REPLAY_FILES\n",
    "replay_files = os.environ.get(\"REPLAY_FILES\")\n",
    "if not replay_files or not replay_files.strip():\n",
    "    raise RuntimeError(\n",
    "        \"REPLAY_FILES environment variable is not set. \"\n",
    "        \"Please specify the files to replay (comma-separated, located in src/file-replay/files).\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build Required Docker Services\n",
    "\n",
    "### Note on `docker_scripts`\n",
    "\n",
    "In this notebook, we use a small helper module called `docker_scripts` to make running Docker and Docker Compose commands easier within a Jupyter environment. This module simply takes a shell command (like `docker build ...` or `docker compose ...`), runs it, and prints the output in a more readable way for notebook users.\n",
    "\n",
    "**You do not need to use `docker_scripts` if you are running commands in a terminal.** For normal development or deployment, just use the standard Docker and Docker Compose commands as shown in the README.\n",
    "\n",
    "The following cells use `docker_scripts` only to improve the notebook experience.\n",
    "\n",
    "First, we'll build the images needed for the context-aware RAG system:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import scripts used to manage docker containers\n",
    "from docker_scripts import tail_bash_command, wait_for_service, docker_ps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build RAG Backend"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#14 39.71  + sniffio==1.3.1\n",
      "#14 39.71  + sqlalchemy==2.0.43\n",
      "#14 39.71  + starlette==0.46.2\n",
      "#14 39.71  + tenacity==8.5.0\n",
      "#14 39.71  + tiktoken==0.11.0\n",
      "#14 39.71  + tqdm==4.67.1\n",
      "#14 39.71  + typing-extensions==4.15.0\n",
      "#14 39.71  + typing-inspect==0.9.0\n",
      "#14 39.71  + typing-inspection==0.4.1\n",
      "#14 39.71  + tzdata==2025.2\n",
      "#14 39.71  + ujson==5.11.0\n",
      "#14 39.71  + urllib3==2.5.0\n",
      "#14 39.71  + uvicorn==0.30.6\n",
      "#14 39.71  + vss-ctx-rag==0.5.1rc5 (from file:///app/dist/vss_ctx_rag-0.5.1rc5-py3-none-any.whl)\n",
      "#14 39.71  + wrapt==1.17.3\n",
      "#14 39.71  + yarl==1.20.1\n",
      "#14 39.71  + zipp==3.23.0\n",
      "#14 DONE 40.1s\n",
      "\n",
      "#15 exporting to image\n",
      "#15 exporting layers\n",
      "#15 exporting layers 2.9s done\n",
      "#15 writing image sha256:053b6aa06de81550e95ebe69a9ee01120e35662326528b7ac6beabc19b141a05 done\n",
      "#15 naming to docker.io/library/ctx_rag 0.0s done\n",
      "#15 DONE 2.9s\n",
      "‚úÖ Done\n"
     ]
    }
   ],
   "source": [
    "# Build the Context-Aware RAG docker image\n",
    "tail_bash_command(\n",
    "    \"docker build -t ctx_rag \"\n",
    "    \"-f ../external/context-aware-rag/docker/Dockerfile \"\n",
    "    \"../external/context-aware-rag\",\n",
    "    n=25\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Done\n"
     ]
    }
   ],
   "source": [
    "# Build remaining images used by Context-Aware RAG workflow\n",
    "tail_bash_command(\n",
    "    \"docker compose \"\n",
    "    \"-f ../external/context-aware-rag/docker/deploy/compose.yaml \"\n",
    "    \"build\",\n",
    "    n=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build Ingestion Workflow\n",
    "\n",
    "Now, we build the images for the FM radio ingestion workflow:\n",
    "- Parakeet ASR NIM (`asr-nim`)\n",
    "- Holoscan-based SDR (`holscan-sdr`)\n",
    "- NeMo Agent Toolkit UI (`agentiq-ui`)\n",
    "- File replay (`fm-file-replay`)\n",
    "\n",
    "This could take up to 10 minutes as images are pulled onto the machine and built."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "#49 128.3   - Updating numba (0.57.1+1.gc785c8f1f /rapids/numba-0.57.1+1.gc785c8f1f-cp310-cp310-linux_x86_64.whl -> 0.60.0)\n",
      "#49 128.3   - Updating pooch (1.7.0 -> 1.8.2)\n",
      "#49 128.3   - Updating scikit-learn (1.2.0 /rapids/scikit_learn-1.2.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl -> 1.6.1)\n",
      "#49 128.3   - Updating soundfile (0.12.1 -> 0.13.1)\n",
      "#49 128.3   - Installing soxr (0.5.0.post1)\n",
      "#49 128.3   - Downgrading typing-extensions (4.15.0 -> 4.14.0)\n",
      "#49 138.9   - Updating cupy-cuda12x (12.1.0 /rapids/cupy_cuda12x-12.1.0-cp310-cp310-linux_x86_64.whl -> 13.4.1)\n",
      "#49 138.9   - Updating librosa (0.9.2 -> 0.11.0)\n",
      "#49 DONE 143.7s\n",
      "\n",
      "#53 [replay 5/5] WORKDIR /workspace\n",
      "#53 DONE 0.1s\n",
      "\n",
      "#54 [replay] exporting to image\n",
      "#54 exporting layers\n",
      "#54 exporting layers 2.4s done\n",
      "#54 writing image sha256:58b9f7a480914d1a7534699db752db32df2b9f4c7818e858b0e2ebdeca4886e9 done\n",
      "#54 naming to docker.io/library/fm-file-replay:latest 0.0s done\n",
      "#54 DONE 2.4s\n",
      "\n",
      "#55 [replay] resolving provenance for metadata file\n",
      "#55 DONE 0.0s\n",
      " agentiq-ui  Built\n",
      " holoscan-sdr  Built\n",
      " replay  Built\n",
      "‚úÖ Done\n"
     ]
    }
   ],
   "source": [
    "# Build containers used in ingestion workflow\n",
    "tail_bash_command(\n",
    "    \"docker compose \"\n",
    "    \"-f ../deploy/docker-compose.yaml \"\n",
    "    \"--profile replay build\",\n",
    "    n=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deployment\n",
    "\n",
    "Now we'll deploy the services needed to run the blueprint. We start by first deploying the context-aware RAG services, then once those are running, the FM radio ingestion pipeline.\n",
    "\n",
    "At any point, containers can be spun down with the following command:\n",
    "```bash\n",
    "docker compose \\\n",
    "    -f ../external/context-aware-rag/docker/deploy/compose.yaml \\\n",
    "    -f ../deploy/docker-compose.yaml --profile replay \\\n",
    "    down\n",
    "```\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b>\n",
    "    \n",
    "We use helper scripts from `docker_scripts` to display Docker Compose output and logs in the notebook. This is only necessary because Docker Compose's native STDOUT printing does not display well in Jupyter cells. \n",
    "\n",
    "These helpers simply make the logs viewable in the notebook environment. If you are running these commands in your own terminal, you should use the standard Docker CLI commands.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Context-Aware RAG\n",
    "\n",
    "Note that the embedding NIM is deployed locally, while the LLM points to a cloud-hosted endpoint specified in `external/context-aware-rag/config/config.yaml --> chat.llm.base_url`.\n",
    "\n",
    "<img src=\"../docs/arch-diagram-retrieval.png\" alt=\"Retrieval Diagram\" style=\"max-width: 600px; width: 100%;\" />"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Container cassandra  Started\n",
      " Container cassandra  Waiting\n",
      " Container grafana  Started\n",
      " Container vss-ctx-rag-retriever  Started\n",
      " Container milvus-etcd  Started\n",
      " Container milvus-minio  Started\n",
      " Container milvus-standalone  Starting\n",
      " Container prometheus  Started\n",
      " Container embedding-nim  Started\n",
      " Container milvus-standalone  Started\n",
      " Container milvus-standalone  Waiting\n",
      " Container milvus-standalone  Healthy\n",
      " Container vss-ctx-rag-data-ingestion  Starting\n",
      " Container vss-ctx-rag-data-ingestion  Started\n",
      " Container cassandra  Healthy\n",
      " Container cassandra_schema  Starting\n",
      " Container cassandra_schema  Started\n",
      " Container cassandra  Waiting\n",
      " Container cassandra  Healthy\n",
      " Container jaeger  Starting\n",
      " Container jaeger  Started\n",
      " Container jaeger  Waiting\n",
      " Container jaeger  Healthy\n",
      " Container otel_collector  Starting\n",
      " Container otel_collector  Started\n",
      "‚úÖ Done\n"
     ]
    }
   ],
   "source": [
    "# Validate ENV\n",
    "api_key = os.environ.get(\"NVIDIA_API_KEY\")\n",
    "assert api_key is not None and api_key != \"nvapi-your-api-key\", \"NVIDIA_API_KEY environment variable is not set\"\n",
    "\n",
    "# Deploy all Context-Aware RAG containers\n",
    "tail_bash_command(\n",
    "    \"docker compose \"\n",
    "    \"-f ../external/context-aware-rag/docker/deploy/compose.yaml \"\n",
    "    \"up -d\",\n",
    "    n=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note: In case of ‚ùå errors or \"unhealthy\" services!</b>\n",
    "    \n",
    "There are other containers deployed by Context-Aware RAG for telemetry and vector RAG, which we do not use in this walkthough. Feel free to disregard errors that are not for one of:\n",
    "1. vss-ctx-rag-retriever\n",
    "2. vss-ctx-rag-data-ingestion\n",
    "3. embedding-nim\n",
    "4. neo4j\n",
    "5. milvus\n",
    "\n",
    "Telemetry services are: `jaeger`, `cassandra`, `cassandra-schema`, `otel_collector`, `prometheus`, and `grafana`, any errors or \"unhealthy\" notifications from those services can be ignored without impacting core functionality.\n",
    "</div>\n",
    "\n",
    "\n",
    "We'll run a function that will wait for each of the required services to send a healthy signal before moving on to deploying the FM radio ingestion workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úÖ Service 'CA RAG Retrieval' is ready\n",
      "‚úÖ Service 'CA RAG Ingestion' is ready\n",
      "‚úÖ Service 'Neo4j' is ready\n",
      "‚úÖ Service 'Milvus' is ready\n",
      "‚úÖ Service 'Embeddings' is ready\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for required services to be ready and healthy\n",
    "wait_for_service(\"http://localhost:8000/health\", name=\"CA RAG Retrieval\")\n",
    "wait_for_service(\"http://localhost:8001/health\", name=\"CA RAG Ingestion\")\n",
    "wait_for_service(\"http://localhost:7474\", name=\"Neo4j\")\n",
    "wait_for_service(\"http://localhost:9091/healthz\", name=\"Milvus\")\n",
    "wait_for_service(\"http://localhost:8002/v1/health/ready\", name=\"Embeddings\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note: In case of Embeddings timeout ‚è≥</b>\n",
    "    \n",
    "If the Embeddings container does not start successfully, make sure that you correctly set your NVIDIA_API_KEY in the setup cell at the top of the notebook. If `!echo $NVIDIA_API_KEY` shows no output or does not display your API key, spin down all containers (see last cell), restart the Jupyter kernel, set your key, and start again.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note: What to do if other services fail to start ‚ùå</b>\n",
    "    \n",
    "If any other of the services above fail to start, re-running the deployment cell is often enough to fix things. Simply re-running won't impact any healthy containers, it just gives one's that had trouble starting another chance.\n",
    "\n",
    "If that still doesn't work, check out the logs during startup with `tail_bash_command(\"docker logs <service-name>\", n=25)` to watch for obvious errors.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the above cell has run, the RAG workflow should be running! Let's check the container status:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   NAMES                        STATUS\n",
      "b6b3ae138b45   otel_collector               Up About a minute\n",
      "c6f2b06f24e6   jaeger                       Up About a minute (healthy)\n",
      "a84dbeb6bfb1   milvus-standalone            Up About a minute (healthy)\n",
      "edbca0350a18   embedding-nim                Up About a minute\n",
      "c5d8a4dc2cce   milvus-minio                 Up About a minute (healthy)\n",
      "989691bdda45   prometheus                   Up About a minute\n",
      "1aae6038b8ef   vss-ctx-rag-retriever        Up About a minute\n",
      "badbde575665   grafana                      Up 2 seconds (health: starting)\n",
      "a5bba80cc891   vss-ctx-rag-data-ingestion   Up About a minute\n",
      "1b13cec7000d   milvus-etcd                  Up About a minute (healthy)\n",
      "6187bebbb526   cassandra                    Up About a minute (healthy)\n",
      "3a80a85caa10   neo4j                        Up About a minute\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docker_ps()  # Print all running containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something similar to:\n",
    "```\n",
    "CONTAINER ID   NAMES                        STATUS\n",
    "e242f3243245   otel_collector               Up About a minute\n",
    "72b7d2678424   jaeger                       Up About a minute (healthy)\n",
    "e00d263b5dac   milvus-standalone            Up About a minute (healthy)\n",
    "0a3eb9eebb30   milvus-minio                 Up About a minute (healthy)\n",
    "48eaec9020ec   embedding-nim                Up About a minute\n",
    "6dd52a851429   neo4j                        Up About a minute\n",
    "72872988d2f5   cassandra                    Up About a minute (healthy)\n",
    "715f6178a6c4   vss-ctx-rag-retriever        Up About a minute\n",
    "c5049894c769   prometheus                   Up About a minute\n",
    "35ee497ada16   grafana                      Up About a minute\n",
    "82d3f9573cb8   milvus-etcd                  Up About a minute (healthy)\n",
    "77df529fa73e   vss-ctx-rag-data-ingestion   Up About a minute\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FM Radio Ingestion Workflow and UI\n",
    "\n",
    "We deploy the containers needed to run the ingestion workflow and for the UI, again using Python subprocesses.\n",
    "\n",
    "Once these containers have spun up, go to [http://localhost:3000](http://localhost:3000) or \"http://\\<your-brev-ip\\>:3000\" in your brower to view and interact with the UI.\n",
    "\n",
    "<img src=\"../docs/arch-diagram-ingestion.png\" alt=\"Ingestion Diagram\" style=\"max-width: 600px; width: 100%;\" />\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note: The first time you deploy ‚è≥</b>\n",
    "   \n",
    "The first time the ingestion workflow is deployed, the ASR NIM will need to download model weights, which will take a few minutes. The weights are cached in the `MODEL_DIRECTORY` folder that we set above so that subsequent deployments are much faster.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time=\"2025-09-25T12:16:35-04:00\" level=warning msg=\"Found orphan containers ([otel_collector vss-ctx-rag-data-ingestion jaeger cassandra_schema milvus-standalone embedding-nim grafana milvus-minio vss-ctx-rag-retriever prometheus cassandra milvus-etcd neo4j]) for this project. If you removed or renamed this service in your compose file, you can run this command with the --remove-orphans flag to clean it up.\"\n",
      " Container holoscan-sdr  Creating\n",
      " Container agentiq-ui  Creating\n",
      " Container fm-file-replay  Creating\n",
      " Container asr-nim  Creating\n",
      " Container agentiq-ui  Created\n",
      " Container holoscan-sdr  Created\n",
      " Container fm-file-replay  Created\n",
      " Container asr-nim  Created\n",
      " Container holoscan-sdr  Starting\n",
      " Container agentiq-ui  Starting\n",
      " Container fm-file-replay  Starting\n",
      " Container asr-nim  Starting\n",
      " Container agentiq-ui  Started\n",
      " Container fm-file-replay  Started\n",
      " Container holoscan-sdr  Started\n",
      " Container asr-nim  Started\n",
      "‚úÖ Done\n"
     ]
    }
   ],
   "source": [
    "# Validate ENV\n",
    "api_key = os.environ.get(\"NVIDIA_API_KEY\")\n",
    "assert api_key is not None and api_key != \"nvapi-your-api-key\", \"NVIDIA_API_KEY environment variable is not set\"\n",
    "assert os.environ.get(\"REPLAY_FILES\") is not None, \"REPLAY_FILES environment variable is not set\"\n",
    "\n",
    "max_file_config = float(os.environ.get(\"REPLAY_MAX_FILE_SIZE\", 50))\n",
    "assert max_file_config < 1000, \\\n",
    "    f\"REPLAY_MAX_FILE_SIZE is recommended to be less than 1 GB (currently {max_file_config/1e3:0.6f} GB). \" \\\n",
    "    f\"You can bypass this check by commenting out this assertion, however, be mindful of the \" \\\n",
    "    \"available GPU memory, which can be checked with `nvidia-smi` from the command line.\"\n",
    "\n",
    "# Deploy all containers used in ingestion workflow\n",
    "tail_bash_command(\n",
    "    \"docker compose \"\n",
    "    \"-f ../deploy/docker-compose.yaml \"\n",
    "    \"--profile replay up -d\",\n",
    "    n=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ASR NIM typically takes the longest to spin up (see note above), we'll wait for that service to send a healthy signal:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚è≥ Waiting 10 seconds for service 'ASR NIM'...\n",
      "‚è≥ Waiting 10 seconds for service 'ASR NIM'...\n",
      "‚è≥ Waiting 10 seconds for service 'ASR NIM'...\n",
      "‚úÖ Service 'ASR NIM' is ready\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Wait for ASR NIM to be ready\n",
    "wait_for_service(\"http://localhost:50050/v1/health/ready\", name=\"ASR NIM\", timeout=600, interval=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b>\n",
    "\n",
    "If using Brev, go to \"http://\\<your-brev-ip\\>:3000\". Find your Brev instance's IP on the instance page on [brev.nvidia.com](brev.nvidia.com).\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll need to wait a minute or so for those services to spin up.\n",
    "\n",
    "Once they have, see the container status below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   NAMES                        STATUS\n",
      "07e88a43ff7e   asr-nim                      Up 31 seconds\n",
      "9d531e9a63dd   holoscan-sdr                 Up 31 seconds\n",
      "6cde989449fc   fm-file-replay               Up 31 seconds\n",
      "7404104de866   agentiq-ui                   Up 31 seconds\n",
      "b6b3ae138b45   otel_collector               Up About a minute\n",
      "c6f2b06f24e6   jaeger                       Up About a minute (healthy)\n",
      "a84dbeb6bfb1   milvus-standalone            Up About a minute (healthy)\n",
      "edbca0350a18   embedding-nim                Up About a minute\n",
      "c5d8a4dc2cce   milvus-minio                 Up About a minute (healthy)\n",
      "989691bdda45   prometheus                   Up About a minute\n",
      "1aae6038b8ef   vss-ctx-rag-retriever        Up About a minute\n",
      "badbde575665   grafana                      Up 6 seconds (health: starting)\n",
      "a5bba80cc891   vss-ctx-rag-data-ingestion   Up About a minute\n",
      "1b13cec7000d   milvus-etcd                  Up About a minute (healthy)\n",
      "6187bebbb526   cassandra                    Up About a minute (healthy)\n",
      "3a80a85caa10   neo4j                        Up About a minute\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docker_ps()  # Print all running containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You should see something similar to:\n",
    "```\n",
    "CONTAINER ID   NAMES                        STATUS\n",
    "71f2de42d1a3   asr-nim                      Up 2 minutes\n",
    "9bef5ebf2abe   holoscan-sdr                 Up 2 minutes\n",
    "b41e8615cff1   fm-file-replay               Up 2 minutes\n",
    "4de614fed065   agentiq-ui                   Up 2 minutes\n",
    "e242f3243245   otel_collector               Up 5 minutes\n",
    "72b7d2678424   jaeger                       Up 5 minutes (healthy)\n",
    "e00d263b5dac   milvus-standalone            Up 5 minutes (healthy)\n",
    "0a3eb9eebb30   milvus-minio                 Up 5 minutes (healthy)\n",
    "48eaec9020ec   embedding-nim                Up 5 minutes\n",
    "6dd52a851429   neo4j                        Up 5 minutes\n",
    "72872988d2f5   cassandra                    Up 5 minutes (healthy)\n",
    "715f6178a6c4   vss-ctx-rag-retriever        Up 5 minutes\n",
    "c5049894c769   prometheus                   Up 5 minutes\n",
    "35ee497ada16   grafana                      Up 5 minutes (healthy)\n",
    "82d3f9573cb8   milvus-etcd                  Up 5 minutes (healthy)\n",
    "77df529fa73e   vss-ctx-rag-data-ingestion   Up 5 minutes\n",
    "```"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Interacting\n",
    "\n",
    "View the frontend at `http://<your-brev-ip>:3000` or [http://localhost:3000](http://localhost:3000). You may have to refresh the page once or twice as the services start up.\n",
    "\n",
    "If using Brev, you can find `<your-brev-ip>` on the instance page on [brev.nvidia.com](brev.nvidia.com).\n",
    "\n",
    "You should see something like this:\n",
    "\n",
    "<img src=\"../docs/ui-example.jpg\" alt=\"Frontend Example\" style=\"max-width: 600px; width: 100%;\" />\n",
    "\n",
    "To view the complete history of transcripts exported to the CA-RAG workflow, click the \"History\" button in the header tab:\n",
    "\n",
    "<img src=\"../docs/ui-header-history.jpg\" alt=\"History Button\" style=\"max-width: 600px; width: 100%;\" />\n",
    "\n",
    "<div class=\"alert alert-block alert-success\">\n",
    "    <b>Tip:</b>\n",
    "\n",
    "When stopping / restarting containers, make sure to refresh the frontend.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note about latency on document ingest\n",
    "\n",
    "Documents are *not* ingested into the database right away. They are batched into sets of N documents; once a batch is full it is processed and subsequently injected into the graph DB.\n",
    "\n",
    "You will have to wait for documents to be ingested into the database before being able to ask questions about them.\n",
    "\n",
    "The batch size is parameterized in `external/context-aware-rag/config/config.yaml --> chat.params.batch_size`. Other parameters for chat are set here as well, including the maximum documents retrieved (`top_k`, with a default of 25). Keep in mind that for smaller batch sizes, the graph formation may not be able to keep up.\n",
    "\n",
    "When the SDR workflow finalizes a transcript and sends it to the ingestion service, it will appear in the transcript history page with a header that looks similar to:\n",
    "\n",
    "<img src=\"../docs/pending-document.jpg\" alt=\"Pending Document\" style=\"max-width: 300px; width: 100%;\" />\n",
    "\n",
    "Once the document's batch is processed and available for retrieval, the header will change to:\n",
    "\n",
    "<img src=\"../docs/ingested-document.jpg\" alt=\"Ingested Document\" style=\"max-width: 300px; width: 100%;\" />\n",
    "\n",
    "At this point, the document is available to be retrieved by the RAG workflow. See the image below; the top entry is not yet accessible for retrieval, while the bottom is:\n",
    "\n",
    "<img src=\"../docs/transcript-history.jpg\" alt=\"Transcript History\" style=\"max-width: 600px; width: 100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sample Questions\n",
    "\n",
    "The context-aware RAG workflow has the ability to filter and retrieve data based on the document ingestion time and document stream. Note that all times are in UTC - to see the relative time, check the timestamps on the transcript history page.\n",
    "\n",
    "For best results, be sure to include the relevant channel(s) and a time constraint in your query ‚Äî for example, ‚Äúchannel 1 at 6:10 AM,‚Äù ‚Äú5 minutes ago in channel 0,‚Äù or ‚Äúa a 10-minute window around 9:00 PM in channels 1 and 2.‚Äù\n",
    "\n",
    "Some suggested questions to start with:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recent summary:\n",
    "\n",
    "+ *\"Summarize the last 10 minutes on channel 0\"*\n",
    "    - Retrieves documents added on stream 0 between 600 seconds ago and now\n",
    "+ *\"Summarize the main topics discussed, excluding channel 2, for the past hour.\"*\n",
    "    - Retrieves up to `top_k` documents from all streams but stream 2 between 3600 seconds ago and now"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Time window:\n",
    "\n",
    "+ *\"What was the topic of conversation on channel 2 15 minutes ago?\"*\n",
    "    - Retrieves documents added on stream 2 900 seconds ago, with a default 5 minute window\n",
    "+ *\"Between 2 minutes and half an hour ago, what was the most interesting fact you heard?\"*\n",
    "    - Retrieves documents between 120 and 1800 seconds ago on all channels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Specific time:\n",
    "+ *\"At 9 oclock, what was the topic on channel 3?\"*\n",
    "    - Retrieves documents in a default 5 minute window around either 9 AM or 9 PM, whichever was more recent\n",
    "+ *\"At 10:00 PM, what was the topic on channel 3, using a 10 minute window?\"*\n",
    "    - Retrieves documents in a 10 minute window around 10 PM on channel 3"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Excluding recent entries\n",
    "+ *\"What was the main topic of conversation on channel 0, excluding the past ten minutes?\"*\n",
    "    - Retrieves up to `top_k` documents on channel 0 added prior to 600 seconds ago"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Troubleshooting\n",
    "\n",
    "The most common error is an incorrect HTTP URL for the Chat Completion call, which may default to `http://127.0.0.1:8000/call`. If this is occuring, the error will likely look something like this:\n",
    "\n",
    "<img src=\"../docs/chat-error.jpg\" alt=\"Chat Error\" style=\"max-width: 500px; width: 100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Go to `Settings -> HTTP URL for Chat Completion` and ensure it is set to `http://vss-ctx-rag-retriever:8000/call`:\n",
    "\n",
    "<img src=\"../docs/settings.jpg\" alt=\"Settings\" style=\"max-width: 200px; width: 100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Then verify clicking \"Regenerate Response\":\n",
    "\n",
    "<img src=\"../docs/chat-success.jpg\" alt=\"Chat Success\" style=\"max-width: 500px; width: 100%;\" />"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Spinning Down\n",
    "\n",
    "To end all services, run the cell below:\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-info\">\n",
    "    <b>Note:</b>\n",
    "\n",
    "If any containers fail to spin down after a reasonable period of time, interrupt the Jupyter process and run: `sudo systemctl restart docker.socket docker.service`.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Container cassandra  Removing\n",
      " Container cassandra  Removed\n",
      " Container fm-file-replay  Stopped\n",
      " Container fm-file-replay  Removing\n",
      " Container holoscan-sdr  Stopped\n",
      " Container holoscan-sdr  Removing\n",
      " Container fm-file-replay  Removed\n",
      " Container holoscan-sdr  Removed\n",
      " Container vss-ctx-rag-data-ingestion  Stopped\n",
      " Container vss-ctx-rag-data-ingestion  Removing\n",
      " Container vss-ctx-rag-data-ingestion  Removed\n",
      " Container neo4j  Stopped\n",
      " Container neo4j  Removing\n",
      " Container neo4j  Removed\n",
      " Container embedding-nim  Stopped\n",
      " Container embedding-nim  Removing\n",
      " Container embedding-nim  Removed\n",
      " Container asr-nim  Stopped\n",
      " Container asr-nim  Removing\n",
      " Container asr-nim  Removed\n",
      " Container grafana  Stopped\n",
      " Container grafana  Removing\n",
      " Container grafana  Removed\n",
      " Network ctx-rag  Removing\n",
      " Network ctx-rag  Removed\n",
      "‚úÖ Done\n"
     ]
    }
   ],
   "source": [
    "# Stop all containers\n",
    "tail_bash_command(\n",
    "    \"docker compose \"\n",
    "    \"-f ../external/context-aware-rag/docker/deploy/compose.yaml \"\n",
    "    \"-f ../deploy/docker-compose.yaml --profile replay \"\n",
    "    \"down\",\n",
    "    n=25\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Verify that all services are stopped:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID   NAMES     STATUS\n",
      "\n"
     ]
    }
   ],
   "source": [
    "docker_ps()  # Print all running containers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Steps\n",
    "\n",
    "Now that you've completed the quickstart, you can explore the codebase and start building your own extensions!\n",
    "\n",
    "- **View the source code and documentation:**  \n",
    "  Visit the [Streaming Data to RAG GitHub repository](https://github.com/NVIDIA-AI-Blueprints/streaming-data-to-rag) to browse the code, open issues, and read more detailed docs.\n",
    "\n",
    "- **Clone the repository:**  \n",
    "  You can clone the repo locally to experiment and develop your own features:\n",
    "  ```\n",
    "  git clone git@github.com:NVIDIA-AI-Blueprints/streaming-data-to-rag.git\n",
    "  ```\n",
    "\n",
    "- **Extend:**  \n",
    "  Feel free to fork the repository and use it as a starting point for your own streaming RAG applications.\n",
    "\n",
    "For more information, see the [README](https://github.com/NVIDIA-AI-Blueprints/streaming-data-to-rag#readme) and the [API integration guide](https://github.com/NVIDIA-AI-Blueprints/streaming-data-to-rag/blob/main/api-integration.md).\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
